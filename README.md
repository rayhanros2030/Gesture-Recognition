# ğŸ¯ Gesture Recognition

[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.5+-green.svg)](https://opencv.org/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.8+-orange.svg)](https://mediapipe.dev/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

A comprehensive computer vision project for real-time static and dynamic gesture recognition using machine learning. Includes a ready-to-use demo and full ML pipeline with model training capabilities.

---

## ğŸš€ Quick Start (Demo)

**Want to see it in action immediately?** Try our zero-setup demo!

```bash
# 1. Install dependencies
python setup_demo.py

# 2. Run the demo
python demo_gesture_detection.py

# 3. Show your hand to the camera!
```

**What you'll see:**
- âœ¨ Real-time hand detection with 21 landmark points
- ğŸ”¢ Finger counting (0-5 fingers)
- ğŸ·ï¸ Automatic gesture naming
- ğŸ“º Visual feedback and bounding boxes
- âŒ **No models or training required!**

ğŸ‘‰ **[See Quick Start Guide](QUICK_START_DEMO.md)** for detailed instructions

---

## ğŸ“– Table of Contents

- [Features](#-features)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
  - [Demo Mode](#demo-mode-quick-test)
  - [Full Project](#full-project-advanced)
- [Architecture](#-architecture)
- [Training Models](#-training-models)
- [Documentation](#-documentation)
- [Requirements](#-requirements)
- [Contributing](#-contributing)
- [License](#-license)

---

## âœ¨ Features

### Demo Features
- ğŸ¥ Real-time webcam hand detection using MediaPipe
- ğŸ¨ Visual landmarks and gesture overlay
- ğŸ“Š Simple finger counting algorithm
- ğŸ–±ï¸ Interactive gesture control
- ğŸŒ Cross-platform support (Windows, macOS, Linux)

### Full Project Features
- ğŸ¤– **Static Gesture Recognition**: Random Forest classifier with MediaPipe landmarks
- ğŸ¬ **Dynamic Gesture Recognition**: Temporal 1D CNN with MobileNet V2 features
- ğŸ“¹ **Real-time Video Processing**: Live gesture detection and control
- ğŸ¯ **System Integration**: Trigger actions based on gestures
- ğŸ”§ **Model Training**: Complete pipeline for custom models
- ğŸ“Š **Data Collection**: Tools for capturing gesture datasets
- ğŸ§ª **Validation**: Dataset validation and model evaluation

---

## ğŸ“ Project Structure

```
Gesture-Recognition/
â”‚
â”œâ”€â”€ ğŸ¯ DEMO FILES (Start Here!)
â”‚   â”œâ”€â”€ demo_gesture_detection.py      # Main demo - works immediately!
â”‚   â”œâ”€â”€ setup_demo.py                  # Auto-install dependencies
â”‚   â”œâ”€â”€ START_HERE.md                  # Quick overview
â”‚   â”œâ”€â”€ QUICK_START_DEMO.md           # Detailed quick start
â”‚   â””â”€â”€ DEMO_README.md                # Demo documentation
â”‚
â”œâ”€â”€ ğŸ¤– TRAINING & MODELS
â”‚   â”œâ”€â”€ 1DCNN_Training_Model.py        # Train dynamic gesture models
â”‚   â”œâ”€â”€ RandomForestTraining.py        # Train static gesture models
â”‚   â”œâ”€â”€ Model_Altogether.py            # Complete gesture control
â”‚   â”œâ”€â”€ config.py                      # Configuration settings
â”‚   â””â”€â”€ run_pipeline.sh               # Full pipeline execution
â”‚
â”œâ”€â”€ ğŸ“Š DATA COLLECTION
â”‚   â”œâ”€â”€ Data_StaticGestures.py         # Capture static gestures
â”‚   â”œâ”€â”€ Data_DynamicGestures.py        # Capture dynamic sequences
â”‚   â”œâ”€â”€ Validate_StaticGestures.py     # Validate static dataset
â”‚   â””â”€â”€ Validate_DynamicGestures.py    # Validate dynamic dataset
â”‚
â”œâ”€â”€ ğŸ“‚ SRC/                            # Source code
â”‚   â”œâ”€â”€ Activity 1/                    # Beginner exercises
â”‚   â”œâ”€â”€ Activity 2/                    # Intermediate projects
â”‚   â”œâ”€â”€ Activity 3/                    # Advanced features
â”‚   â”œâ”€â”€ SampleCode/                    # Code examples
â”‚   â””â”€â”€ EXTRA FILES/                   # Additional resources
â”‚
â”œâ”€â”€ ğŸ“¦ DATA/                           # Datasets and models
â”‚   â””â”€â”€ haarcascades/                  # OpenCV cascade files
â”‚
â”œâ”€â”€ ğŸ§ª TESTS/                          # Test scripts
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ğŸ“‹ DEPENDENCIES
â”‚   â”œâ”€â”€ requirements.txt               # Full project dependencies
â”‚   â””â”€â”€ .gitignore                     # Git ignore rules
â”‚
â””â”€â”€ ğŸ“š DOCUMENTATION
    â”œâ”€â”€ README.md                      # This file
    â”œâ”€â”€ LICENSE                        # License file
    â””â”€â”€ DEMO_FILES_SUMMARY.txt         # Technical overview
```

---

## ğŸ’» Installation

### For Demo Only (Recommended First)

**Option 1: Automatic Setup**
```bash
python setup_demo.py
python demo_gesture_detection.py
```

**Option 2: Manual Setup**
```bash
pip install opencv-python mediapipe numpy
python demo_gesture_detection.py
```

### For Full Project

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/Gesture-Recognition.git
cd Gesture-Recognition

# Install all dependencies
pip install -r requirements.txt

# Run full pipeline
./run_pipeline.sh
```

---

## ğŸ® Usage

### Demo Mode (Quick Test)

Perfect for beginners or quick demonstrations. No training required!

```bash
python demo_gesture_detection.py
```

**Supported Gestures:**
- âœŠ Fist (0 fingers)
- â˜ï¸ One (1 finger)
- âœŒï¸ Two / Peace (2 fingers)
- ğŸ¤Ÿ Three (3 fingers)
- ğŸ–– Four (4 fingers)
- ğŸ–ï¸ Open Hand (5 fingers)

**Controls:**
- Show hand to camera â†’ See detection
- Press `q` â†’ Quit

---

### Full Project (Advanced)

#### 1. Data Collection

**Static Gestures:**
```bash
python Data_StaticGestures.py
# Press 's' to capture screenshots
```

**Dynamic Gestures:**
```bash
python Data_DynamicGestures.py
# Records 65 frames automatically
```

#### 2. Model Training

**Train Static Model:**
```bash
python RandomForestTraining.py
```

**Train Dynamic Model:**
```bash
python 1DCNN_Training_Model.py
```

#### 3. Real-time Recognition

```bash
python Model_Altogether.py
```

**Features:**
- Static gesture recognition
- Dynamic gesture sequences
- Mode switching (Thumbsup gesture)
- System action triggers

---

## ğŸ—ï¸ Architecture

### Static Gesture Recognition

```
Webcam â†’ MediaPipe â†’ Landmark Extraction â†’ Random Forest â†’ Gesture Label
```

- **Input**: Single frame from webcam
- **Processing**: MediaPipe extracts 21 hand landmarks
- **Classification**: Random Forest trained on landmark features
- **Output**: Gesture label (Thumbsup, TwoFingers, Handup, etc.)

### Dynamic Gesture Recognition

```
Video Sequence â†’ MobileNet V2 â†’ Temporal 1D CNN â†’ Gesture Classification
```

- **Input**: 65-frame video sequence
- **Feature Extraction**: MobileNet V2 (pre-trained on ImageNet)
- **Temporal Modeling**: 1D Convolutional layers
- **Output**: Gesture classification

### Model Specifications

| Model | Input | Architecture | Parameters |
|-------|-------|--------------|------------|
| Static | 21Ã—3 landmarks | Random Forest | 100 estimators |
| Dynamic | 65 frames @ 224Ã—224 | MobileNet V2 + 1D CNN | ~3.5M |

---

## ğŸ§ª Training Models

### Static Gesture Training

```python
# Data structure:
Gesture_Dataset_Static/
â”œâ”€â”€ Thumbsup/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ TwoFingers/
â”‚   â””â”€â”€ ...
â””â”€â”€ Handup/
    â””â”€â”€ ...

# Train:
python RandomForestTraining.py

# Output:
# - static_landmark_model.pkl
# - static_label_encoder.pkl
```

### Dynamic Gesture Training

```python
# Data structure:
Gesture_Dataset_Dynamic/
â”œâ”€â”€ Gesture1/
â”‚   â”œâ”€â”€ Seq_1/
â”‚   â”‚   â”œâ”€â”€ frame_000.jpg
â”‚   â”‚   â”œâ”€â”€ frame_001.jpg
â”‚   â”‚   â””â”€â”€ ... (65 frames)
â”‚   â””â”€â”€ Seq_2/
â”‚       â””â”€â”€ ...
â”œâ”€â”€ Gesture2/
â”‚   â””â”€â”€ ...

# Train:
python 1DCNN_Training_Model.py

# Configuration (config.py):
SEQUENCE_LENGTH = 65
IMAGE_SIZE = 224
BATCH_SIZE = 4
NUM_EPOCHS = 25

# Output:
# - gesture_temporal_cnn_model.pth
```

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [README.md](README.md) | Project overview (this file) |
| [QUICK_START_DEMO.md](QUICK_START_DEMO.md) | Fast setup guide |
| [DEMO_README.md](DEMO_README.md) | Detailed demo documentation |
| [START_HERE.md](START_HERE.md) | Navigation guide |
| [DEMO_FILES_SUMMARY.txt](DEMO_FILES_SUMMARY.txt) | Technical overview |
| [DEMO_COMPLETE.txt](DEMO_COMPLETE.txt) | Feature summary |

---

## ğŸ“‹ Requirements

### Demo Requirements
- Python 3.7+
- opencv-python >= 4.5.0
- mediapipe >= 0.8.0
- numpy >= 1.21.0

### Full Project Requirements
See [requirements.txt](requirements.txt) for complete list:
- Core: numpy, opencv-python, pandas, matplotlib
- ML: scikit-learn, joblib
- Deep Learning: torch, torchvision, tensorflow, keras
- CV: mediapipe, Pillow
- Utilities: tqdm

---

## ğŸ“ Learning Path

### Beginner
1. âœ… Start with `demo_gesture_detection.py`
2. âœ… Understand MediaPipe basics
3. âœ… Modify finger counting logic
4. âœ… Read code comments

### Intermediate
1. âœ… Collect static gesture data
2. âœ… Train Random Forest model
3. âœ… Validate dataset quality
4. âœ… Test recognition accuracy

### Advanced
1. âœ… Capture dynamic sequences
2. âœ… Train CNN temporal model
3. âœ… Implement gesture control
4. âœ… Add custom gestures

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### How to Contribute
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **MediaPipe** by Google for hand detection
- **OpenCV** community for computer vision tools
- **PyTorch** team for deep learning framework
- Contributors and users of this project

---

## ğŸ“§ Contact

Have questions or suggestions? Feel free to open an issue or reach out!

---

<div align="center">
  <h3>â­ Star this repo if you found it helpful! â­</h3>
  <p>Built with â¤ï¸ using Python, OpenCV, MediaPipe, and PyTorch</p>
</div>
